{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Zadanie 4 (7 pkt)\n","Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n","\n","* Implementacja funkcji entropii - **0.5 pkt**\n","* Implementacja funkcji entropii zbioru - **0.5 pkt**\n","* Implementacja funkcji information gain - **0.5 pkt**\n","* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n","* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"],"metadata":{"id":"cpar5LziY_-0"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","import math\n","from collections import Counter\n","import numpy as np\n","\n","iris = load_iris()\n","\n","x = iris.data\n","y = iris.target\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"],"metadata":{"id":"XNc-O3npA-J9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Group:\n","    def __init__(self, group_classes):\n","        self.group_classes = group_classes\n","        self.entropy = self.group_entropy()\n","\n","    def __len__(self):\n","        return self.group_classes.size\n","\n","class Node:\n","    def __init__(self, split_feature=None, split_val=None, child_node_a=None, child_node_b=None, val=None, depth=None):\n","        self.split_feature = split_feature \n","        self.split_val = split_val \n","        self.depth = depth \n","        self.child_node_a = child_node_a \n","        self.child_node_b = child_node_b \n","        self.val = val \n","\n","    def predict(self, data, tree):\n","        return np.array([self.travers_tree(x, tree) for x in data]) #twórze listę, podaje x i buduje drzewo\n","\n","    def travers_tree(self, x, tree): #metoda rekurencyjna, która będzie przewidywać na podstawie drzewa\n","        if tree.is_leaf_node(): #jeśli to węzeł końcowy, zwracamy wartość\n","            return tree.val\n","            \n","        if x[tree.split_feature] <= tree.split_val: #jeśli nie końcowy, patrzymy gdzie iść w lewo lub w prawo\n","            return self.travers_tree(x, tree.child_node_a) #wlewo\n","        return self.travers_tree(x, tree.child_node_b) #wprawo\n","\n","    def is_leaf_node(self): #metoda sprawdzania na węzł końcowy\n","        return self.val is not None\n","\n","class DecisionTreeClassifier(object):\n","    def __init__(self, max_depth):\n","        self.depth = 0 #glebokosc\n","        self.max_depth = max_depth #max glebokosc\n","        self.min_samples = 10 # min ilosc samples\n","        self.tree = None #dzewo\n","\n","    def fit(self, x_train, y_train): #metoda nauki, budowania drzewa\n","        self.tree = self.build_tree(x_train, y_train) #wywołana metoda build_tree, aby zbudować drzewo\n","\n","    def entropy(self, y): #entropia\n","        hist = np.bincount(y) #definicja częstotliwości klas\n","        ps = hist / len(y) # obliczanie udziału każdej klasy\n","        return -np.sum([p * np.log2(p) for p in ps if p > 0]) # obliczenie entropii\n","\n","    def get_information_gain(self, parent_group, child_group_a, child_group_b): # Obliczanie information gain\n","        if len(np.unique(child_group_a)) == 1: #jeśli w węźle zostało tylko jedno miejsce, zatrzymujemy się wtedy zawartość informacji wynosi 0\n","            return 0\n","        \n","        n = len(child_group_a) #określanie liczbę obserwacji w węźle nadrzędnym\n","        parent = self.entropy(y) #entropia perents wezla\n","       \n","        #określienie wskaźnikow obserwacji, które przeszły na lewą i prawa stronę. \n","        #flatten() zamienia kolumnę na str\n","        left_indexes = np.argwhere(parent_group <= child_group_b).flatten() \n","        right_indexes = np.argwhere(parent_group > child_group_b).flatten() \n","        \n","        #obliczanie entropii po podzieleniu w lewym i prawym węźle\n","        e_l, n_l = self.entropy(child_group_a[left_indexes]), len(left_indexes) \n","        e_r, n_r = self.entropy(child_group_a[right_indexes]), len(right_indexes) \n","        \n","        #entropia po podzieleniu \n","        child = (n_l / n) * e_l + (n_r / n) * e_r \n","        return parent - child #zwracam information_gain\n","\n","    def get_best_feature_split(self, feature_values, classes): #metoda efektywnej separacji features\n","        labels = np.unique(classes) \n","        count = [list(classes).count(i) for i in labels] #liczenie każdej labels\n","        return labels[np.argmax(count)] #zwracam najbardziej czesty labels\n","\n","    def get_best_split(self, data, classes): #metoda najefektywnej separacji \n","        best_feature, best_threshold = None, None #najlepszy feature and threshold\n","        best_gain = -1              \n","        \n","        #algorytm prochodzienia po wszystkiem cecham \n","        for i in range(data.shape[1]): \n","            thresholds = np.unique(data[:, i]) \n","            for threshold in thresholds: \n","                gain = self.get_information_gain(data[:, i], classes, threshold)\n","                if gain > best_gain: \n","                    best_gain = gain \n","                    best_feature = i \n","                    best_threshold = threshold \n","        return best_feature, best_threshold \n","\n","    def build_tree(self, data, classes, depth=0): # rekurencyjne budowanie drzewa\n","        n_samples, n_features = data.shape \n","        n_labels = len(np.unique(classes)) #sprawdzanie, ile labels zostało, aby uniknąć sytuacji, w której została jedna label\n","        \n","        if n_samples <= self.min_samples or depth >= self.max_depth or n_labels == 1: ## jeśli n_samples to minimalna dozwolona wartość lub głębokość to maksymalna dozwolona wartość lub n_labels to pozostała jedna klasa\n","            return Node(val=self.get_best_feature_split(0, classes)) \n","        \n","        best_feature, best_threshold = self.get_best_split(data, classes) \n","        \n","        #określa, które podziały poszły w lewo, a które w prawo, w tym celu określamy lewe lub prawe indeksy.\n","        left_indexes = np.argwhere(data[:, best_feature] <= best_threshold).flatten() \n","        right_indexes = np.argwhere(data[:, best_feature] > best_threshold).flatten() \n","        \n","        if len(left_indexes) == 0 or len(right_indexes) == 0: #jeśli lewe lub prawe indeksy są puste\n","            return Node(val=self.get_best_feature_split(0, classes)) #zwracamy w węźle wartość, którą wcześniej obliczyliśmy metodą get_best_feature_split\n","        \n","        #budujemy drzewo \n","        left = self.build_tree(data[left_indexes, :], classes[left_indexes], depth + 1) \n","        right = self.build_tree(data[right_indexes, :], classes[right_indexes], depth + 1) \n","        \n","        return Node(best_feature, best_threshold, left, right) \n","\n","    def predict(self, data): # metoda przewidywania wytrenowanej sieci danych\n","        return self.tree.predict(data, self.tree) \n"],"metadata":{"id":"fBh2tfQ44u5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dc = DecisionTreeClassifier(3)\n","dc.fit(x_train, y_train)\n","\n","prediction = dc.predict(x_test)\n","print(prediction)\n","#print(x_train)\n","np.sum(prediction == y_test) / len(y_test)"],"metadata":{"id":"U033RY1_YS8x","colab":{"base_uri":"https://localhost:8080/"},"outputId":"720bbcf3-685b-461e-cc85-3275b5ad1f47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9333333333333333"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["### Przy ustawieniu test_size=0.1, random_state=123:\n","\n","Dżewo [2 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n","\n","Accuracy 0.9333333333333333\n","\n","\n","---\n","\n","\n","### Przy ustawieniu test_size=0.2:\n","Accuracy 0.9666666666666667\n","\n","Accuracy 1.0\n","\n","Accuracy 0.8333333333333334\n","\n","---\n","### Przy ustawieniu test_size=0.3:\n","\n","Accuracy 0.9777777777777777\n","\n","Accuracy 0.8444444444444444\n","\n","Accuracy 0.9333333333333333\n","\n","\n"],"metadata":{"id":"4eWXlJNPgcoj"}}]}